---
title: 'London spatial analysis (income) lab'
author: 'ELENA SMYSLOVSKIKH'
output:
        html_document:
                highlight: default
                theme: readable
                toc: yes
                number_sections: true
                toc_depth: 5
                toc_float:
                        collapsed: false
                        smooth_scroll: true
---


```{r warning=FALSE, message=FALSE, results='hide'}
#-------------------------Package Installer--------------------------
# load packages and install if missing
# thanks to Richard Schwinn for the original code, http://stackoverflow.com/a/33876492

# list the packages you need
p <- c('data.table', 'ggplot2', 'sf', 'psych', 'reshape2', 'tidyverse',
       'dplyr', 'corrgram', 'spdep', 'mapview', 'tmap', 'ggpubr', 'spatialreg')

# this is a package loading function
loadpacks <- function(package.list = p){
new.packages <- package.list[!(package.list %in% installed.packages()[,'Package'])]
  if(length(new.packages)) {
    install.packages(new.packages, Ncpus = parallel::detectCores(), type = "binary", repos = "https://cran.rstudio.com")
  }
lapply(eval(package.list), require, character.only = TRUE)
}

loadpacks(p) # calling function to load and/or install packages
rm(loadpacks, p) # cleanup namespace
#----------------------End of Package Installer----------------------

#------------------------------Options-------------------------------

data.table::setDTthreads(threads = parallel::detectCores())
options(scipen = 999)

#---------------------------End of Options---------------------------

main <- function() {
    
    
    
}

rm(main)

```


# Code book

You will be using a number of datasets in this lab.

## income_est

This dataset is a summary of income istimates (in british pounds) for 2002-2013 by London boroughs. The variables are as follows:

- **Code** = unique area code
- **Borough** = London borough
- **the rest of the columns are dates** = Total Median Annual Household Income estimate for each year

## qualifications

This dataset is an extract from the full qualifications dataset - few variables are selected form the full data set and the year is 2011. The variables are as follows:

- **Code** = unique area code
- **Borough** = London borough
- **share_qualified** = share of highly qualified people of working age (16-64)
- **share_unqualified** = share of unqualified people of working age (16-64)

# Load data

## Load data from `income_est` file from `data` subfolder into a `data.table` or `tibble` object with a name of your choice.


You might need to tell the function you are using to read the csv file that you have column names in the first row.

```{r}
income <- fread("data/income_est.csv", header=TRUE)
```

## Load `qualifications` file from `data` subfolder into  a `data.table` or `tibble` object with a name of your choice.

You might need to tell the function you are using to read the csv file that you have column names in the first row. For `fread()` it would be a `header = TRUE` parameter.

```{r}
qual <- fread("data/qualifications.csv", header=TRUE)
```



# Get to know the datasets

## Check the structure of the dataset with data from the `income_est` file


```{r}
str(income)
```

## Show first and last three lines of the dataset with data from the `income_est` file

```{r}
headTail(income, top=3, bottom=3)
```


## Check the structure of the dataset with data from the `qualifications` file

```{r}
str(qual)
# no record of city of London
```

## Show first and last three lines of the dataset with data from the `qual` file

```{r}
headTail(qual, top=3, bottom=3)
```

# Transform and explore data

## Transform data from the `income_est` file

Transfrom from the `income_est` file so that it has the following columns:

- **Code** = unique area code
- **Borough** = London borough
- **Year** = year
- **Income** = median income in particular year

You may name the variables as you like.

Save the result to an object, give it any name you like.

```{r}
income_molten <- melt(income, id.vars=c('Code', 'Borough'), 
                      variable.name='Year', value.name = 'income_city')

income_by_year <- income_molten %>% group_by(Year) %>% summarise( MedianIncomeYear = median(income_city) )
```

https://stackoverflow.com/questions/54366592/r-how-to-take-median-of-rows-in-dataframe

```{r}
income_new <- merge(income_molten, income_by_year,
                        by.x='Year', by.y='Year')

names(income_new)[5] <- 'Income'
names(income_new)[4] <- 'Income_est'
#income_new <- income_new[-c(4)]

```

## Print first few rows of the dataset transformed above

```{r}
head(income_new)
```

## Summarize data

Using the dataset you created in the previous step, print top 5 boroughs by max income.


```{r}
income_new_copy <- income_new %>% group_by(Borough) %>% summarise( MedianIncomeBorough = median(Income_est) )

sorted <- arrange(income_new_copy, desc(MedianIncomeBorough))

sorted[1:5,]
```


Using the dataset you created in the previous step, print only 6th to 10th top boroughs by income.


```{r}
sorted[6:10,]
```

## Plot top 10 boroughs

Create a bar plot comparing the top 10 boroughs by income.

Hint 1: use `stat = 'identity'` parameter for bar plot geom.
Hint 2: use `+ coord_flip()` with your plot for better readability.

https://stackoverflow.com/questions/59008974/why-is-stat-identity-necessary-in-geom-bar-in-ggplot

```{r}
sorted[1:10,] %>% ggplot(aes(Borough, MedianIncomeBorough)) +
  geom_bar(stat = "identity") +
  coord_flip()
```


# Enrich the data

Select only one year worth of data from `income_est` (choose the year that corresponds to the year of `qulifications` data) and save it as a new `data.table` or `tibble`. You should keep the `Code`, the `Borough` name and the `income` estimate column.

2011 год

```{r}
income_2011 <- filter(income_new, Year == 2011)
head(income_2011)
```

Merge your new income estimate table with the qualifications table and save the result as a new table (you should get a 32x5 table with Code, Borough, share qualified, share unqualified and estimated income columns, all variables should be for the same year or within 1 year):


```{r}
data <- merge(income_2011, qual, by.x='Code', by.y='Code', all.x=TRUE)
names(data)[3] <- 'Borough'
data <- data %>% select(!c(Borough.y, Year, Income))
head(data)
```


Create a scatter plot for each pair of variables in the new table Or you can create a corrgram for all variables at once if you recall how to do it from lab 6.


```{r}
panel.shadeNtext <- function (x, y, corr = NULL, col.regions, ...) 
{
      corr <- cor(x, y, use = "pair")
      results <- cor.test(x, y, alternative = "two.sided")
      est <- results$p.value
      stars <- ifelse(est < 5e-4, "***", 
                      ifelse(est < 5e-3, "**", 
                             ifelse(est < 5e-2, "*", "")))
      ncol <- 14
      pal <- col.regions(ncol)
      col.ind <- as.numeric(cut(corr, breaks = seq(from = -1, to = 1, 
                                                   length = ncol + 1), include.lowest = TRUE))
      usr <- par("usr")
      rect(usr[1], usr[3], usr[2], usr[4], col = pal[col.ind], 
           border = NA)
      box(col = "lightgray")
      on.exit(par(usr))
      par(usr = c(0, 1, 0, 1))
      r <- formatC(corr, digits = 2, format = "f")
      cex.cor <- .8/strwidth("-X.xx")
      fonts <- ifelse(stars != "", 2,1)
      # option 1: stars:
      text(0.5, 0.4, paste0(r,"\n", stars), cex = cex.cor)
      # option 2: bolding:
      #text(0.5, 0.5, r, cex = cex.cor, font=fonts)
}
```

```{r,fig.width=5, fig.height=5}
corrgram(data, upper.panel = panel.pts, lower.panel = panel.shadeNtext, diag.panel = panel.density)
```

_Expectedly, a higher proportion of skilled workers is significantly positively associated with neighborhood income, while a higher proportion of unskilled workers is associated with higher income._

Test all variables for normality. Do you need to log-transform any variables? You may choose to answer that using plots or empirically (by trying to build models).

```{r,fig.width=4, fig.height=4}
hist(data$Income_est, breaks=33)
```

```{r}
shapiro.test(data$Income_est)
```

_The income variable has to be transformed._


```{r,fig.width=4, fig.height=4}
hist(data$share_qualified, breaks=33)
```


```{r}
shapiro.test(data$share_qualified)
```

```{r,fig.width=4, fig.height=4}
hist(data$share_unqualified, breaks=33)
```


```{r}
shapiro.test(data$share_unqualified)
```

_The p-value in the Shapiro test is greater than the critical value, so we can say that we have no significant differences from the normal distribution. You can also see on the correlogram plots that the distribution is close to normal._

__


Build a multiple linear regression model. Try to predict income by the shares of qualified and unqualified workforce.

```{r}
m.1 <- lm(log(Income_est) ~ share_qualified + share_unqualified, data=data)
```


Print model summary:
```{r}
summary(m.1)
```

Interpret the model by analysing the residuals and inspecting the key model summary coefficients.

```{r,fig.width=7, fig.height=7}
ggResidpanel::resid_panel(m.1)
```

_The residuals do not depend on predicated values, and we can also see from the QQ-plot that the range of values is close to the line. This tells us that the model has sufficient predictive power._

Apart from code, provide a brief description of how you can interpret the results.

_It can be noticed that both coefficients are significant. Like in the correlations, the increase in the share of qualified workers increases district income on average, ceteris paribus. In contrast, the increase in the share of unqualified labor workers decreases district income on average, ceteris paribus._

Do you think the model may suffer from spatial autocorrelation? You will be able to explore this later in the lab.

# Working with spatial data

## Load spatial data

Load `london_boroughs` spatial data from `data` sub folder to object with any name. Make sure it has the same number of borouhgs as your table from the previous part of the analysis.

```{r}
st_layers('data/london_boroughs.gpkg')
```

```{r}
london <- st_read('data/london_boroughs.gpkg', layer = 'london_24')
```

## Plot a map of boroughs boundaries you just loaded

```{r}
london %>% st_geometry() %>% plot(lwd = 0.7)
```


```{r}
mapview(london)
```

## Check the spatial object's data

### Check the variable names of the spatial data set


```{r}
names(london)
```

### Check the structure of the spatial data set

```{r}
str(london)
```


## Subset your molten dataset

## Merge qulification data with spatial data

Merge/join your table (the one that you used for modelling above) to the spatial object. Show the structure of the resulting spatial object.

```{r}
spatial_dat <- merge(london, data, by.x = "GSS_CODE", by.y = 'Code', all.x=TRUE)
```


```{r}
str(spatial_dat)
```


## Plot spatial data

Create a map showing income. Use ggplot, tmap or mapview, or any package you want.

```{r}
mapview(spatial_dat, zcol='Income_est')
```


_We can see that lower income districts are located in the North-Eastern part of the city, while higher income districts are in the city centre and South-East. Therefore we may be dealing with spatial autocorrelation._

# Spatial models

Now you have an option to try and improve the regression model that you have built previously. Does the model suffer from spatial autocorrelation? You may add additional code blocks to structure your code and make it cleaner.

_Let us find the neighbours for each district._

```{r}
neighbours <- spdep::poly2nb(spatial_dat)
neighbours
```

_Let us construct the matrix of neghbours by using the initial sf-object._

_**Queen** neighborhood type_

```{r}
lnd_nb_queen <- poly2nb(spatial_dat, queen = TRUE)
plot.nb(lnd_nb_queen, st_geometry(spatial_dat), lwd = 0.3)
```

_**Rook** neighborhood type_

```{r}
lnd_nb_rook <- poly2nb(spatial_dat, queen = FALSE)
plot.nb(lnd_nb_rook, st_geometry(spatial_dat), lwd = 0.3)
```

_We may estimate the number of neighbours for each district via two ways and compare in which case the variation will be lower._

```{r}
n_nbrs_queen <- sapply(lnd_nb_queen, length)
n_nbrs_rook <- sapply(lnd_nb_rook, length)
```

```{r}
compare_nbrs <- data.table(queen = n_nbrs_queen, rook = n_nbrs_rook)
```

```{r}
compare_nbrs_long <- melt(compare_nbrs, measure.vars = c("queen", "rook"), value.name = "n_neighbours", variable.name = "type")
```


```{r}
compare_nbrs_long %>%
  ggplot() +
  geom_histogram(aes(x = n_neighbours, col = type), fill = NA, binwidth = 1, position = "identity") +
  facet_wrap(~type)+
  theme_pubclean()
```

_The distribution of neighbours is identical irregardless of neighborhood type._

```{r}
weights <- nb2listw(lnd_nb_queen, style="W")
```

```{r}
spatial_dat$lag_Income_est <- lag.listw(x = weights, spatial_dat$Income_est)
```


```{r}
moran.mc(spatial_dat$Income_est, listw = weights, nsim = 999)
```

_According to the p-value (which is lower than the significance level), there is spatial autocorrelation of income between the districts._

```{r}
local_moran_Income_est <- localmoran(x = spatial_dat$Income_est, listw = weights)
hist(local_moran_Income_est[,5], breaks = 20)
```

```{r}
signif <- 0.05
spatial_dat$Income_est_sig <- local_moran_Income_est[,5] # get the 5th column of the matrix - the `Pr(z > 0)` column
spatial_dat$Income_est_li <- local_moran_Income_est[,1] - mean(local_moran_Income_est[,1])

spatial_dat <- spatial_dat %>% mutate(quad_sig_Income_est = case_when(Income_est_sig > signif ~ "non-significant",
                                                      Income_est > 0 & lag_Income_est > 0 & Income_est_sig <= signif ~ "high-high",
                                              Income_est < 0 & lag_Income_est < 0 & Income_est_sig <= signif ~ "low-low",
                                              Income_est > 0 & lag_Income_est < 0 & Income_est_sig <= signif ~ "high-low",
                                              Income_est < 0 & lag_Income_est > 0 & Income_est_sig <= signif ~ "low-high"))

table(spatial_dat$quad_sig_Income_est)

```

```{r}
head(spatial_dat)
```

_We can see that the significance is added to the last column of the dataset._

```{r}
map_1 <- spatial_dat %>% dplyr::select(Income_est, Income_est_sig, quad_sig_Income_est)
mapview(map_1, zcol = "quad_sig_Income_est")

```

_The only significantly different districts are the central district of London and some of the South-Eastern districts._


```{r}
system.time( man_lagsarlm <- lagsarlm(log(Income_est) ~ share_qualified + share_unqualified, data = spatial_dat, listw = weights) )
```

```{r}
summary(man_lagsarlm)
```

```{r}
system.time( man_errorsarlm <- errorsarlm(log(Income_est) ~ share_qualified + share_unqualified, data = spatial_dat, listw = weights) )
```

```{r}
summary(man_errorsarlm)
```

_Коэффициент автокорреляции ошибок - 0.24087._


# Write a function

Write a simple function with two parameters.

This function must take as an sf object as a first parameter. The second parameter is a character vector of length one with the name of the variable that you want to create a histogram for. The function log-transforms the variable specified via the 2nd parameter and creates a histogram with title "Log-transformed variable histogram".

If you cannot write such function, write a function that finds a 5th root of the product of squares of 3 numbers - all passed as arguments into the function, and the result should be divided by some number passed as the fourth argument to that function. The second and third arguments default value is 1. The first argument has no default value. The output of the function is a printout like `You have entered numbers 3, 7 and 8. The result rounded to 2 digits is: 7.76`. In any case, try to build the function in its basic form. You may not, for example, be able to create a proper printout for the function, but the result of the calculation may be correct.

Define your function below:

```{r}
fun_1 <- function(sf_obj,
                  var) {
  sf_obj <- sf_obj %>% st_set_geometry(NULL)
  var_vector = log(sf_obj[,var])
  hist_toplot <- hist(var_vector, main="Log-transformed variable histogram", breaks=33)
  return(hist_toplot)
}

```

Test your function three times with different values for parameters in the code chunk below:

```{r}
fun_1(spatial_dat, 'Income_est')
```

```{r}
fun_1(spatial_dat, 'share_qualified')
```

```{r}
fun_1(spatial_dat, 'share_unqualified')
```
## Error checking

Rewrite your function in such a way that it checks if the name of the variable to log-transform and creates a histogram that you are passing to it actually exists in the dataset. If it does not exist, return and error message with `print()` instead of trying to plot the variable that does not exist.

```{r}
fun_1 <- function(sf_obj,
                  var) {
  if(var %in% colnames(sf_obj)){
  sf_obj <- sf_obj %>% st_set_geometry(NULL)
  var_vector = log(sf_obj[,var])
  hist_toplot <- hist(var_vector, main="Log-transformed variable histogram", breaks=33)
  return(hist_toplot)
                  }
  else {
    return(print('Error! This variable does not exist'))
  }
}
```

```{r}
fun_1(spatial_dat, 'ty')
```

<!-- This is a style section, do not delete it. -->
<!-- Heading 5 will be used for grader's remarks. -->

<style>
        h5 {
                color: red;
                font-family: Arial,Helvetica Neue,Helvetica,sans-serif;
        }
</style>

<br>
<!-- ------------------------------------------------- -->














